// Flashcards 001 - מושגי יסוד בלמידה עמוקה
export default {
  id: "flashcards-001",
  name: "מושגי יסוד בלמידה עמוקה",
  description: "כרטיסיות לחזרה על מושגים בסיסיים",
  cards: [
    {
      front: "מהו Overfitting?",
      back: "התאמת יתר - כאשר המודל לומד את נתוני האימון \"בעל פה\" ולא מצליח להכליל לנתונים חדשים. המודל מראה ביצועים מעולים על קבוצת האימון אך ביצועים גרועים על קבוצת הבדיקה."
    },
    {
      front: "מהו Underfitting?",
      back: "התאמת חסר - כאשר המודל פשוט מדי ולא מצליח ללמוד את המבנים בנתונים. ביצועים גרועים גם על קבוצת האימון וגם על קבוצת הבדיקה."
    },
    {
      front: "מהי פונקציית אקטיבציה?",
      back: "פונקציה לא-ליניארית שמופעלת על הפלט של נוירון. מאפשרת לרשת ללמוד קשרים מורכבים ולא-ליניאריים. דוגמאות: ReLU, Sigmoid, Tanh."
    },
    {
      front: "מתי משתמשים ב-Sigmoid?",
      back: "בעיקר בשכבת הפלט עבור סיווג בינארי, כי היא מחזירה ערך בין 0 ל-1 שניתן לפרש כהסתברות."
    },
    {
      front: "מהו ReLU?",
      back: "Rectified Linear Unit - פונקציית אקטיבציה: f(x) = max(0, x). יתרון: פשוטה לחישוב ומונעת בעיית vanishing gradients. חיסרון: Dead Neurons - נוירונים שהפלט שלהם תמיד 0."
    },
    {
      front: "מהו Leaky ReLU?",
      back: "וריאציה של ReLU שפותרת את בעיית Dead Neurons: f(x) = max(αx, x) כאשר α הוא קבוע קטן (למשל 0.01). מאפשרת גרדיאנט קטן גם לערכים שליליים."
    },
    {
      front: "מהו Dropout?",
      back: "טכניקת רגולריזציה שבה בכל איטרציה של האימון, נוירונים אקראיים \"מבוטלים\" עם הסתברות מסוימת. מונע co-adaptation ומדמה אנסמבל של מודלים."
    },
    {
      front: "מהו Batch Normalization?",
      back: "טכניקה לנרמול הפלטים של שכבה לפני העברתם לשכבה הבאה. מייצבת את האימון, מאפשרת learning rate גבוה יותר, ומסייעת בבעיית vanishing/exploding gradients."
    },
    {
      front: "מהו Epoch?",
      back: "מעבר אחד על כל הדוגמאות בקבוצת האימון. לדוגמה: אם יש 60,000 דוגמאות ו-mini-batch בגודל 256, אז epoch אחד = 235 איטרציות."
    },
    {
      front: "מהו Mini-batch?",
      back: "קבוצה קטנה של דגימות מאוסף הנתונים שעליה מחשבים את הגרדיאנט בכל איטרציה של SGD. מאזן בין דיוק הגרדיאנט (batch גדול) לבין מהירות ויכולת לצאת ממינימום מקומי (batch קטן)."
    },
    {
      front: "מהו Softmax?",
      back: "פונקציה שממירה וקטור של מספרים לוקטור הסתברויות (חיוביים שסכומם 1).\nנוסחה: softmax(zᵢ) = e^zᵢ / Σe^zⱼ\nמשמשת בשכבת פלט לסיווג רב-מחלקתי."
    },
    {
      front: "מה התכונות של פונקציית Softmax?",
      back: "1. מניבה מספרים חיוביים שסכומם 1\n2. אינוואריאנטית לתוספת קבוע\n3. פונקציה גזירה\n4. אינה מושפעת מסדר האיברים\n*לא* מחזירה ערכים בינאריים!"
    },
    {
      front: "מהו Cross Entropy Loss?",
      back: "פונקציית מחיר לסיווג: C = -Σyₜlog(y)\nכאשר yₜ הסיווג האמיתי ו-y הסתברות המודל. מעניישת סיווג שגוי בצורה פרופורציונלית ל-log של ההסתברות השגויה."
    },
    {
      front: "מה היחס בין Sigmoid ל-Tanh?",
      back: "tanh(x) = 2σ(2x) - 1\nכאשר σ היא sigmoid.\nניתן להמיר רשת עם sigmoid ל-tanh על ידי הקטנת המשקלים פי 2 בכניסה ופי 2 ביציאה והתאמת ההטיות."
    },
    {
      front: "מהי בעיית Vanishing Gradients?",
      back: "בעיה ברשתות עמוקות שבה הגרדיאנטים נהיים קטנים מאוד בשכבות המוקדמות, מה שמונע למידה. נפוצה עם sigmoid/tanh. פתרונות: ReLU, Batch Normalization, Residual Connections."
    },
    {
      front: "מהי בעיית Exploding Gradients?",
      back: "בעיה שבה הגרדיאנטים גדלים באופן לא יציב דרך השכבות, גורמים לעדכוני משקל קיצוניים ואי-התכנסות. פתרונות: Gradient Clipping, Batch Normalization, אתחול משקלים נכון."
    },
    {
      front: "מהם Dead Neurons?",
      back: "נוירונים שהפלט שלהם תמיד 0 ולא מתעדכנים יותר. קורה ב-ReLU כאשר הקלט תמיד שלילי. פתרון: Leaky ReLU שמאפשרת גרדיאנט קטן גם לערכים שליליים."
    },
    {
      front: "מהו Condition Number?",
      back: "יחס בין הערך העצמי הגדול ביותר לקטן ביותר של מטריצת ההסיאן. condition number גבוה = התכנסות איטית של Gradient Descent בגלל עדכונים קטנים בכיוון הערך העצמי הקטן."
    },
    {
      front: "מהו מומנטום (Momentum)?",
      back: "שיפור ל-SGD שמוסיף \"תנופה\" לעדכון המשקלים. מחזק תנועות עקביות בכיוון מסוים ומחליש תנודות חדות. עוזר לצאת מאזורים שטוחים ולהאיץ התכנסות."
    },
    {
      front: "מהו Adam Optimizer?",
      back: "אופטימייזר שמשלב מומנטום עם adaptive learning rate לכל פרמטר בנפרד. מתאים את קצב הלמידה לפי היסטוריית הגרדיאנטים. פופולרי מאוד בפרקטיקה."
    },
    {
      front: "מהו RMSprop?",
      back: "אופטימייזר עם קצב למידה אדפטיבי שמתאים את גודל העדכון לכל פרמטר לפי הגרדיאנטים ההיסטוריים שלו. דומה ל-Adam אך ללא מומנטום."
    },
    {
      front: "מהו Learning Rate Scheduling?",
      back: "שינוי דינמי של קצב הלמידה במהלך האימון. בדרך כלל מתחילים גבוה ומקטינים עם הזמן, כדי לאפשר קפיצות קטנות יותר קרוב להתכנסות."
    },
    {
      front: "כיצד מחשבים Accuracy?",
      back: "Accuracy = מספר הסיווגים הנכונים / סה\"כ דגימות\nבסיווג רב-מחלקתי: משווים argmax של הפלט לתווית האמיתית."
    },
    {
      front: "מהו Backpropagation?",
      back: "אלגוריתם לחישוב גרדיאנטים ברשת נוירונים. משתמש בכלל השרשרת (chain rule) כדי לחשב את הנגזרת של פונקציית המחיר ביחס לכל משקל, משכבת הפלט לכיוון הקלט."
    },
    {
      front: "מהו Chain Rule בהקשר של רשתות?",
      back: "כלל לחישוב נגזרות של הרכבת פונקציות:\n∂L/∂w = ∂L/∂y · ∂y/∂z · ∂z/∂w\nמאפשר לחשב את הגרדיאנט שכבה-שכבה."
    },
    {
      front: "מהו Fully Connected Layer?",
      back: "שכבה שבה כל נוירון מחובר לכל הנוירונים בשכבה הקודמת. מחשבת: z = Wx + b. נקראת גם Dense Layer."
    },
    {
      front: "מדוע צריך פונקציית אקטיבציה לא-ליניארית?",
      back: "בלי אי-ליניאריות, כל הרשת שקולה לטרנספורמציה ליניארית אחת (כפל מטריצות). אי-ליניאריות מאפשרת ללמוד קשרים מורכבים ופונקציות לא-מונוטוניות."
    },
    {
      front: "מהו Weight Decay?",
      back: "שם נוסף ל-L2 רגולריזציה. מוסיף λ||w||² לפונקציית העלות, מה שגורם למשקלים להישאר קטנים ומונע overfitting."
    },
    {
      front: "מתי Dropout *לא* פועל?",
      back: "בשלב ה-Test/Inference. בזמן אימון משמיטים נוירונים, אך בזמן בדיקה משתמשים בכל הנוירונים עם משקלים מותאמים (מוכפלים ב-keep probability)."
    },
    {
      front: "מה ההבדל בין Training Set ל-Validation Set?",
      back: "Training Set: נתונים שעליהם מאמנים את המודל.\nValidation Set: נתונים לבדיקת ביצועים במהלך האימון (לכיוון hyperparameters).\nTest Set: נתונים להערכה סופית בלבד."
    },
    {
      front: "מהי השפעת גודל Mini-batch?",
      back: "קטן: עדכונים תכופים, רעש גבוה (עוזר לצאת ממינימום מקומי), זיכרון נמוך.\nגדול: גרדיאנט מדויק, התכנסות יציבה יותר, צריך יותר זיכרון, עלול להיתקע במינימום מקומי."
    },
    {
      front: "מהו Saddle Point?",
      back: "נקודה שבה הגרדיאנט = 0 אך אינה מינימום או מקסימום (מינימום בכיוון אחד, מקסימום באחר). Gradient Descent עלול להתקע בה."
    },
    {
      front: "מהו משפט הקירוב האוניברסלי?",
      back: "רשת עם שכבה חבויה אחת יכולה לקרב כל פונקציה רציפה בדיוק כרצוננו. אך בפועל משתמשים ברשתות עמוקות כי: 1) קל יותר לאמן 2) דרושים פחות נוירונים 3) ייצוגים היררכיים."
    },
    {
      front: "כמה פרמטרים ב-FC Layer?",
      back: "שכבה עם n קלטים ו-m פלטים:\nמשקלים: n×m\nהטיות: m\nסה\"כ: n×m + m = m(n+1)"
    },
    {
      front: "מהי Weight Initialization?",
      back: "אתחול המשקלים ההתחלתיים של הרשת. אתחול לא נכון גורם לבעיות אימון. שיטות נפוצות: Xavier, He. המטרה: לשמור על variance קבוע דרך השכבות."
    },
    {
      front: "מהו Residual Connection (Skip Connection)?",
      back: "חיבור שמדלג על שכבות ומוסיף את הקלט ישירות לפלט: y = F(x) + x. מאפשר אימון רשתות עמוקות מאוד (100+ שכבות) ופותר vanishing gradients."
    },
    {
      front: "מהו Co-adaptation?",
      back: "תופעה שבה נוירונים מתמחים יחד ותלויים זה בזה. Dropout מונע זאת על ידי כיבוי אקראי - כל נוירון נאלץ ללמוד features שימושיים באופן עצמאי."
    }
  ]
};
