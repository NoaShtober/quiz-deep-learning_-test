// Test 002 - מועד א׳ 1 - יולי 2025 (מועד 86)
export default {
  id: "test-001",
  name: "מועד א׳ 1 - יולי 2025",
  description: "20 שאלות מהמבחן הרשמי",
  questions: [
    {
      question: "איזו מהתשובות הבאות נכונה ביותר עבור פעולת אלגוריתם מורד הגרדיאנט (gradient descent) לתהליך המינימיזציה של פונקציית המחיר (cost function).",
      options: [
        "בכל נקודה במרחב הפרמטרים, תנועה קטנה בכיוון השלילי של הגרדיאנט תוביל לירידה בערך פונקציית המחיר.",
        "אם גודל הצעד (learning rate) קטן מדי, האלגוריתם עשוי להתקדם באיטיות רבה ואף לא להתכנס גם לאחר מספר רב של איטרציות.",
        "גודל צעד גדול מדי יוביל לאי-יציבות בתהליך וייתכן שנדלג מעל נקודת המינימום, וכך ערך פונקציית המחיר לא ייירד.",
        "כל התשובות נכונות."
      ],
      correct: 3
    },
    {
      question: "נתונות הטענות הבאות עבור פונקציית ה-softmax:\n1. מניבה מספרים חיוביים שסכומם אחת.\n2. אינוואריאנטית לתוספת קבוע לכל אחד מהאיברים, כלומר softmax(x₁+c, x₂+c, ..., xₙ+c) = softmax(x₁, x₂, ..., xₙ).\n3. פונקציה גזירה.\n4. תמיד מחזירה ערכים בינאריים (0 או 1).\n5. אינה מושפעת מסדר האיברים בוקטור הקלט.\n\nבחרו בתשובה הנכונה ביותר:",
      options: [
        "אף טענה אינה נכונה.",
        "רק טענה אחת נכונה.",
        "רק שתי טענות נכונות.",
        "רק שלוש טענות נכונות.",
        "רק ארבע טענות נכונות.",
        "כל חמש הטענות נכונות."
      ],
      correct: 4,
      explanation: "טענות 1,2,3,5 נכונות. טענה 4 שגויה - softmax מחזירה ערכים רציפים בין 0 ל-1, לא ערכים בינאריים."
    },
    {
      question: "השימוש בשיטת הגרדיאנט האקראי (Stochastic Gradient Descent - SGD) נועד להתמודד עם הבעיה הבאה (בחרו בתשובה הנכונה ביותר):",
      options: [
        "היעדר גרדיאנט בנקודות לא גזירות של פונקציית העלות.",
        "חישוב איטי של הגרדיאנט באימון רשתות מורכבות על אוסף נתונים (dataset) גדול.",
        "חוסר יכולת למצוא מינימום גלובלי של פונקציית העלות.",
        "תשובות א׳, ב׳, ו-ג׳ נכונות.",
        "תשובות א׳, ב׳, ו-ג׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "SGD נועד להאיץ את חישוב הגרדיאנט על ידי שימוש ב-mini-batches במקום כל ה-dataset."
    },
    {
      question: "נתונה רשת נוירונים מסוג Fully connected FeedForward הכוללת שתי שכבות, כאשר פלט הרשת מוגדר ע״י:\ny = h(Uᵀg(Wᵀx + b) + c),\nעם פונקציות אקטיבציה g() בשכבה החבויה ו-h() בשכבת המוצא.\nנתון כי וקטור הקלט הוא x ∈ ℝᵈ, הפרמטרים הנלמדים הם מטריצות המשקלות W ∈ ℝᵈˣᵏ, ו-U ∈ ℝᵏˣᵈ, ווקטורי ההטיות b ∈ ℝᵏ ו-c ∈ ℝᵈ, והפלט הוא y ∈ ℝᵈ. נתון גם ש-k < d.\n\nבחרו בתשובה הנכונה ביותר, בהינתן שפונקציות האקטיבציה הן פונקציות הזהות (identity), כלומר:\ng(z) = z, h(z) = z.",
      options: [
        "רשת זו מסוגלת לייצר כל טרנספורמציה אפינית (affine) מהצורה y = Pᵀx + d בין הקלט x ∈ ℝᵈ לבין הפלט y ∈ ℝᵈ.",
        "רשת זו מאפשרת ייצוג של הקלט x ∈ ℝᵈ באמצעות מרחב נמוך מימד.",
        "הודות לשימוש ביותר משכבה אחת, רשת זו עשויה לייצג קשרים מורכבים יותר מקשרים אפיניים (affine) בין הקלט לפלט.",
        "תשובות א׳, ב׳, ו-ג׳ אינן נכונות.",
        "תשובות א׳, ב׳, ו-ג׳ נכונות."
      ],
      correct: 1,
      explanation: "עם פונקציות זהות, הרשת מבצעת הפחתת מימד (dimensionality reduction) כי k < d."
    },
    {
      question: "מה ההבדל העיקרי בין Stochastic Gradient Descent (SGD) עם מומנטום לבין Adam?",
      options: [
        "SGD עם מומנטום משתמש רק במידע על כיוון הגרדיאנט, בעוד Adam מתאים את כיוון וגודל העדכון לפי גרדיאנטים קודמים.",
        "Adam משלב מומנטום עם התאמה של קצב הלמידה לכל פרמטר בנפרד.",
        "בעוד ש־SGD עם מומנטום משתמש בגרדיאנט ממוצע לאורך זמן, Adam מחשב עדכון מבוסס על סטיית תקן של הגרדיאנטים בלבד.",
        "Adam משתמש רק בגרדיאנט הנוכחי, ללא מידע היסטורי כמו מומנטום."
      ],
      correct: 1,
      explanation: "Adam משלב מומנטום עם adaptive learning rate לכל פרמטר בנפרד."
    },
    {
      question: "מהם היתרונות המרכזיים של רשתות עצביות קונבולוציוניות (Convolutional Neural Networks) בעיבוד תמונות, בהשוואה ל-Fully connected FeedForward networks?",
      options: [
        "רשתות קונבולוציה דורשות יותר פרמטרים ולכן מסוגלות ללמוד תבניות מורכבות יותר.",
        "רשתות קונבולוציה משתמשות בשכבות קונבולוציה המאפשרות שיתוף משקלים וזיהוי תבניות מקומיות, דבר המפחית את מספר הפרמטרים ומשפר את יכולת ההכללה.",
        "רשתות קונבולוציה אינן תלויות בסדר הפיקסלים ולכן אינן מושפעות ממיקום האובייקטים בתמונה.",
        "רשתות קונבולוציה מתעלמות ממבנה התמונה ולכן אינן מנצלות מידע מרחבי לצורך זיהוי תכונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ נכונות.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "שיתוף משקלים וזיהוי תבניות מקומיות הם היתרונות המרכזיים של CNN."
    },
    {
      question: "במסגרת אימון רשת נוירונים לזיהוי עצמים בתמונות, המודל מפגין דיוק גבוה על קבוצת האימון אך ביצועים ירודים על קבוצת הוולידציה. איזו מהאפשרויות הבאות פחות מתאימה להתמודדות עם מצב זה?",
      options: [
        "עצירה אוטומטית של תהליך האימון כאשר אין שיפור מתמשך במדדי הביצוע על סט הוולידציה.",
        "השמטה אקראית של יחידות פנימיות בשכבות הנסתרות במהלך האימון.",
        "שילוב של פלטים ממספר עותקים שונים של הרשת שאומנו בנפרד.",
        "הגדלת מספר שכבות הרשת והארכת זמן האימון עד למיצוי מוחלט של הדאטה.",
        "יצירת גרסאות נוספות של הדאטה באמצעות סיבובים, חיתוכים ושינויים חזותיים."
      ],
      correct: 3,
      explanation: "הגדלת מספר שכבות הרשת והארכת זמן האימון תחמיר את ה-overfitting."
    },
    {
      question: "רשת קונבולוציה מקבלת כקלט טנזור קלט ממימד 256x128 העובר דרך שכבת קונבולוציה בעלת 64 פילטרים עם גודל גרעין 10x6, stride 3x5, וללא ריפוד באפסים. מה יהיה גודל טנזור הפלט?\n\nבחרו את התשובה הנכונה:",
      options: [
        "64x40x50.",
        "64x40x51.",
        "64x39x50.",
        "64x41x52.",
        "תשובות א׳, ב׳, ג׳ ו-ד׳ אינן נכונות."
      ],
      correct: 4,
      explanation: "חישוב: (256-10)/3+1=83, (128-6)/5+1=25.4 → לא מספר שלם, לכן אף תשובה לא נכונה."
    },
    {
      question: "צוות חוקרים מבקש לאמן מודל לזיהוי סוגים נדירים של סרטן באמצעות תמונות מיקרוסקופיות. ברשותם רק 300 תמונות מתויגות, ומספר הקטגוריות לסיווג גבוה יחסית.\n\nבאיזו מהגישות הבאות סביר ביותר להשתמש כדי לשפר את ביצועי המודל בתנאים אלו?",
      options: [
        "לאמן רשת עמוקה מאוד מאפס (from scratch) על סט התמונות הקטן.",
        "להשתמש ב-Transfer Learning עם מודל שאומן מראש על תמונות כלליות (כגון ImageNet) ולבצע fine-tuning.",
        "לשכפל את התמונות הקיימות כדי להרחיב את סט האימון.",
        "לאמן רשת עמוקה מאוד מאפס (from scratch), ולהחיל Dropout על כל שכבות המודל כדי לצמצם תופעת אימון היתר (overfitting)."
      ],
      correct: 1,
      explanation: "Transfer Learning מאפשר להשתמש בידע קיים ממודל שאומן על dataset גדול, ולהתאים אותו למשימה החדשה עם מעט דוגמאות."
    },
    {
      question: "איזו מהדרכים הבאות אינה נחשבת לשיטה מקובלת ליישום Transfer Learning במודל רשת עמוקה?",
      options: [
        "הקפאת (freeze) השכבות המוקדמות במודל שאומן מראש ואימון רק של השכבות האחרונות על הדאטה החדש.",
        "שכפול ארכיטקטורת המודל המאומן מראש, אך אתחול אקראי של כל המשקולות ואימון מחדש על הדאטה החדש כדי למנוע הטיות ממשימה קודמת.",
        "טעינת משקולות ממודל שאומן מראש וביצוע fine-tuning על חלק מהשכבות לפי הדאטה החדש.",
        "החלפת השכבות האחרונות במודל המאומן בשכבות חדשות המותאמות למשימה חדשה."
      ],
      correct: 1,
      explanation: "אתחול אקראי של כל המשקולות מבטל את היתרון של Transfer Learning - העברת הידע מהמודל המאומן."
    },
    {
      question: "בעת תכנון רשת נוירונים, קיימת התאמה טבעית בין סוג הבעיה, פונקציית האקטיבציה בשכבת הפלט, ופונקציית המחיר (cost function). איזו מההתאמות הבאות אינה אידיאלית לסוג הבעיה?",
      options: [
        "בעיית רגרסיה, פונקציית פלט: זהות (identity), פונקציית המחיר: סכום ריבועים.",
        "סיווג בינארי, פונקציית פלט: סיגמואיד, פונקציית המחיר: Cross Entropy.",
        "סיווג רב־מחלקתי (multi-class), פונקציית פלט: softmax, פונקציית המחיר: Cross Entropy רב-מחלקתי.",
        "סיווג בינארי, פונקציית פלט: tanh, פונקציית המחיר: סכום ריבועים."
      ],
      correct: 3,
      explanation: "tanh עם סכום ריבועים אינה התאמה אידיאלית לסיווג בינארי - עדיף להשתמש ב-sigmoid עם Cross Entropy."
    },
    {
      question: "נתונה פונקציה ריבועית קמורה מהצורה:\nf(x) = ½xᵀAx,\nכאשר A היא מטריצה סימטרית וחיובית מוגדרת. נניח כי קיים פער משמעותי בין הערך העצמי הקטן ביותר לערך העצמי הגדול ביותר של A.\n\nאופטימיזציה של הפונקציה מתבצעת באמצעות שיטת מורד הגרדיאנט (Gradient Descent) עם וקטור התחלה שרירותי ועם גודל צעד קבוע ותקין (שאינו גדול מדי כך שהשיטה מתכנסת תאורטית). הסבירו מהו הגורם העיקרי שמשפיע על קצב ההתכנסות של השיטה, ובחרו בתשובה הנכונה ביותר מתוך האפשרויות הבאות:",
      options: [
        "ההתכנסות מהירה בגלל שהגרדיאנטים גדולים בכיוון של הערך העצמי הגדול של A.",
        "ההתכנסות איטית, בעיקר בגלל שהעדכונים בכיוון של הערך העצמי הגדול של A גדולים יותר.",
        "ההתכנסות איטית, בעיקר בגלל שהעדכונים בכיוון של הערך העצמי הקטן של A קטנים יותר.",
        "שיטת מורד הגרדיאנט אינה רגישה כלל לערכים העצמיים של A.",
        "תשובות א׳, ב׳, ג׳ ד׳ ו-ה אינן נכונות."
      ],
      correct: 2,
      explanation: "העדכונים הקטנים בכיוון הערך העצמי הקטן מאטים את ההתכנסות - זו בעיית ה-condition number."
    },
    {
      question: "מהי מטרת השימוש ב־Dataset Augmentation באימון רשתות נוירונים?\n\nבחרו איזו מהתשובות הבאות אינה נכונה.",
      options: [
        "להרחיב את גודל קבוצת האימון על ידי יצירת דוגמאות נוספות מהדאטה הקיים.",
        "לשפר את יכולת ההכללה של המודל ולצמצם סכנת Overfitting.",
        "לקצר את זמן האימון הכולל של המודל, מאחר והמודל רוכש את ההתפלגות הרצויה של הנתונים תוך שימוש בפחות דוגמאות אמיתיות.",
        "לאלץ את המודל ללמוד אינוואריאנטיות מסוימות שהתופעה אמורה להיות אדישה להן."
      ],
      correct: 2,
      explanation: "Data Augmentation בדרך כלל מאריך את זמן האימון כי יש יותר דוגמאות לעבד, לא מקצר אותו."
    },
    {
      question: "מה ההשפעה הצפויה של dropout על זמן האימון הנדרש להתכנסות? בחרו בתשובה הנכונה ביותר.",
      options: [
        "מקצר את זמן האימון מאחר ופחות נוירונים פעילים.",
        "מאריך את זמן האימון, כי בכל איטרציה מאמנים תת-רשת שונה.",
        "אין כל השפעה על זמן האימון.",
        "Dropout מפסיק את האימון ברגע שנוצר overfitting, ולכן מקצר את זמן האימון.",
        "תשובות א׳, ב׳, ג׳, ו-ד׳ אינן נכונות."
      ],
      correct: 1,
      explanation: "Dropout מאריך את זמן האימון כי בכל איטרציה מאמנים תת-רשת שונה, מה שמאט את ההתכנסות."
    },
    {
      question: "שאלות 15,16:\nנתונה פונקציית מחיר (cost function) ריבועית מהצורה:\nf(w) = ½wᵀQw + bᵀw\nכאשר Q ∈ ℝ²ˣ² היא מטריצה סימטרית עם ערכים עצמיים λ₁ = 10, λ₂ = 2.5 ווקטורים עצמיים מתאימים v₁ᵀ = [1,2]/√5, v₂ᵀ = [-2,1]/√5. הוקטור b נתון ע״י bᵀ = [-1,5.5].\n\nשאלה 15:\nחשבו את הנקודה w* המביאה למינימום את פונקצית המחיר.",
      options: [
        "w*ᵀ = [1,-1].",
        "w*ᵀ = [-1,1].",
        "w*ᵀ = [1,0].",
        "w*ᵀ = [0,-1].",
        "תשובות א׳, ב׳, ג׳, ו-ד׳ אינן נכונות."
      ],
      correct: 0
    },
    {
      question: "שאלה 16:\nמוסיפים לפונקציית המחיר איבר רגולריזציה מסוג Ridge:\nf(w) = ½wᵀQw + bᵀw + (λ/2)||w||₂²\nעם פרמטר λ = 10. נקודת המינימום w_reg של פונקציית המחיר החדשה עם איבר הרגולריזציה נתונה ע״י:",
      options: [
        "w_regᵀ ≈ [-0.14,0.32].",
        "w_regᵀ ≈ [-0.14,0.14].",
        "w_regᵀ ≈ [0.14,-0.32].",
        "w_regᵀ ≈ [0.32,-0.14].",
        "w_regᵀ ≈ [-0.32,0.14].",
        "w_regᵀ ≈ [0.14,-0.14]."
      ],
      correct: 0
    },
    {
      question: "מהו היתרון העיקרי של פונקציית האקטיבציה ReLU בהשוואה ל-sigmoid ול-tanh ברשתות נוירונים? בחרו בתשובה הנכונה ביותר.",
      options: [
        "פונקציית ה-ReLU מבטיחה שפלט האקטיבציה יהיה בין 0 ל-1, ולכן מתאימה טוב יותר לבעיות סיווג.",
        "פונקציית ה-ReLU אינה גורמת לעולם לבעיה של גרדיאנט נעלם (vanishing gradients).",
        "פונקציית ה-ReLU חוסכת חישובים ונוטה להוביל להתכנסות מהירה יותר באימון רשתות נוירונים.",
        "פונקציית ה-ReLU משמרת מידע מרחבי טוב יותר."
      ],
      correct: 2,
      explanation: "ReLU פשוטה לחישוב (max(0,x)) ונוטה להוביל להתכנסות מהירה יותר."
    },
    {
      question: "במהלך אימון רשת CNN הכוללת מספר שכבות convolution ו-MaxPooling, אתם מבחינים בכך שמידע מרחבי חשוב הולך לאיבוד כבר בשכבות הראשונות. מה הסיבה הסבירה ביותר לכך?",
      options: [
        "ערך ה-stride בשכבות הקונבולוציה גדול מדי.",
        "פונקציית האקטיבציה ReLU גורמת לאובדן מידע מרחבי ולכן אינה מתאימה למבנה כזה.",
        "החסר בשכבת Fully Connected גורם לכך שהרשת לא מצליחה לשמר מידע בשכבות הראשונות.",
        "אין מספיק training data, ולכן הרשת לא לומדת לשמר את המידע המרחבי החשוב."
      ],
      correct: 0,
      explanation: "stride גדול מדי גורם לאיבוד מידע מרחבי כבר בשכבות הראשונות."
    },
    {
      question: "בעת חישוב הגרדיאנט של פונקציית המחיר (cost function) ביחס לפרמטרים של שכבה מסוימת ברשת נוירונים, אילו רכיבים משפיעים עליו? בחרו בתשובה הנכונה ביותר:",
      options: [
        "רק הקלט שהוזן לשכבה זו.",
        "רק השגיאה שחושבה ביציאת הרשת.",
        "גם הקלט לשכבה וגם הגרדיאנט שהועבר מהשכבה הבאה.",
        "רק גודל הרשת.",
        "תשובות א׳, ב׳, ג׳, ו-ד׳ אינן נכונות."
      ],
      correct: 2,
      explanation: "לפי chain rule ב-backpropagation, הגרדיאנט תלוי גם בקלט לשכבה וגם בגרדיאנט שמגיע מהשכבות הבאות."
    },
    {
      question: "נתונה רשת נוירונים לסיווג בינארי עם שכבה אחת, שבה הפלט מחושב לפי:\nŷ = σ(wᵀx + b),\nכאשר σ היא פונקציית ה-sigmoid, ופונקציית המחיר J היא cross-entropy, כלומר עבור דוגמה בודדת (x, y):\nJ(x, y) = -(y·log(ŷ) + (1-y)·log(1-ŷ))\n\nמהו הגרדיאנט של פונקציית המחיר J(x, y) ביחס לוקטור הפרמטרים w?",
      options: [
        "(2y-1)σ((1-2y)(wᵀx + b))x.",
        "(1-2y)σ((1-2y)(wᵀx + b))x.",
        "(2y-1)(1 - σ((1-2y)(wᵀx + b)))x.",
        "(1-2y)σ((2y-1)(wᵀx + b))x.",
        "תשובות א׳, ב׳, ג׳, ו-ד׳ אינן נכונות."
      ],
      correct: 0
    }
  ]
};
